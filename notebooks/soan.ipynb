{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOcial ANalysis (SOAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**  \n",
    "On 17 August 2018,I married the woman of my dreams. In order to know just how special she was (and is) to me I decided to put my data science skills to work. I analysed whatsapp data and all of our pictures that we made together. The end product was a small book that included results of this notebook with descriptions of all analyses that were done. I gave it to her the day before we married as to show what she means to me. \n",
    "\n",
    "**Project**  \n",
    "The project started off with basic data analyses, namely getting means and sums of different statistics such as average number of words per message per user and eventually evolved into analyzing topics that were frequently discussed in the group chat. Personally, the most interesting analysis I have done in this project was using TF-IDF as a way to extract words that characterize each person in the group chat. It is interesting to see which words are often used by one person, but rarely by all others. \n",
    "\n",
    "Use the table of contents below to go to the analysis of your interest. \n",
    "\n",
    "https://chatvisualizer.com/demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"table\">Table of Contents</a> \n",
    "\n",
    "1. [Functions](#functions)\n",
    "\n",
    "2. [General Statistics](#general)  \n",
    "\n",
    "    2.1 [Prepare Data](#gs-data)  \n",
    "        \n",
    "    2.2 [Users in the Group](#users)\n",
    "    \n",
    "    2.3 [Messages over time](#gs-messages)  \n",
    "    \n",
    "    2.4 [Active Moments](#gs-active) \n",
    "    \n",
    "    2.5 [Averages / Summaries](#gs-averages) \n",
    "    \n",
    "    2.6 [Response Time](#gs-response) \n",
    "    \n",
    "3. [TF-IDF](#tfidf)  \n",
    "\n",
    "    3.1 [Count Vector](#tf-count)  \n",
    "\n",
    "    3.2 [Unique Words](#tf-unique)  \n",
    "\n",
    "    3.3 [Plot Image](#tf-image)  \n",
    "\n",
    "4. [Emoji Analysis](#emoji)  \n",
    "\n",
    "    4.1 [Prepare Data](#emoji-prepare)  \n",
    "\n",
    "    4.2 [Unique and Common Emojis](#emoji-stats)  \n",
    "\n",
    "    4.3 [Correlation Matrix](#emoji-corr)  \n",
    "    \n",
    "5. [Topic Modelling](#topic)  \n",
    "\n",
    "    5.1 [Prepare Data](#topic-prepare)  \n",
    "\n",
    "    5.2 [LDA](#topic-lda)  \n",
    "\n",
    "    5.3 [NMF](#topic-nmf)  \n",
    "    \n",
    "6. [Sentiment Analysis](#sentiment)  \n",
    "\n",
    "    6.1 [Prepare Data](#sentiment-prepare)  \n",
    "\n",
    "    6.2 [Average Sentiment](#sentiment-average)  \n",
    "    \n",
    "    6.3 [Plot Sentiment](#sentiment-plot)  \n",
    "    \n",
    "7. [Word Clouds](#cloud)  \n",
    "\n",
    "    7.1 [Prepare Data](#cloud-prepare)  \n",
    "\n",
    "    7.2 [Masked Word Cloud](#cloud-mask)  \n",
    "\n",
    "    7.3 [Sentiment-based Word Cloud](#cloud-sentiment) \n",
    "    \n",
    "8. [Color Analysis](#color)  \n",
    "\n",
    "    8.1 [Extract Frequent Colors (k-Means)](#color-get)  \n",
    "\n",
    "    8.2 [Plot Frequent Colors](#color-plot)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"functions\">1. Functions</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I purposefully imported all packages manually just to show you which are which and how they work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install palettable\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')  # Needed to import package in parent dir, remove this if you pip installed the package\n",
    "from soan.whatsapp import helper      # Helper to prepare the data\n",
    "from soan.whatsapp import general     # General statistics\n",
    "from soan.whatsapp import tf_idf      # To calculate TF-IDF\n",
    "from soan.whatsapp import emoji       # To analyze emoji use\n",
    "from soan.whatsapp import topic       # Topic modelling and summarization\n",
    "from soan.whatsapp import sentiment   # Sentiment Analysis\n",
    "from soan.whatsapp import wordcloud   # Create Word Clouds\n",
    "\n",
    "from soan.colors   import colors      # Frequent Color Visualization\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"general\">2. General Statistics</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find general data with regards to whatsapp conversations such as the number of messages and words. Keep in mind this is simply to get a general understanding of the conversations. More complex and interesting information can be found in the later chapters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gs-data\">2.1 Prepare Data</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is preprocessed by loading in the raw txt file which is sent through whatsapp to your mail. The following columns are created:\n",
    "* Message_Raw\n",
    "    * The raw message for each user (including date and name of user)\n",
    "* User\n",
    "    * The user who has sent the message, if no users have sent that message\n",
    "    then it is simply removed from the dataframe\n",
    "* Message_Clean\n",
    "    * Only containing the message, but with all possible symbols in there\n",
    "    Thus, no date and no name of the user\n",
    "* Message_Only_Text\n",
    "    * Only text, lowercase\n",
    "* Date\n",
    "    * Date in the format y/m/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper.import_data('/Users/Marius/Downloads/_chat.txt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper.preprocess_data(df)\n",
    "\n",
    "# Change labels for anonymization - Leave this at False\n",
    "if False:\n",
    "    user_labels = {old: new for old, new in zip(sorted(df.User.unique()), ['Her', 'Me'])}\n",
    "    df.User = df.User.map(user_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"users\">2.2 Users in the group</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.print_users(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"Maria Perez Suarez\"\n",
    "language = \"french\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gs-messages\">2.3 Messages over time</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.plot_messages(df, colors=None, trendline=False, savefig=False, dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gs-active\">2.4 Active Moments</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily Activity**  \n",
    "This shows the daily activity per person in a spider plot and in total in a histogram. \n",
    "NOTE: This is unstandardized data, which means that it also shows the amount of messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.plot_day_spider(df, colors=None, savefig=False, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.plot_active_days(df, savefig=False, dpi=100, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.plot_active_days(df, savefig=False, dpi=100, user=\"Marius\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hourly Activity**  \n",
    "It shows which days are most active based on the period in which most messages are send. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.plot_active_hours(df, color='#ffdfba', savefig=False, dpi=100, user='All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.plot_active_hours(df, color='#ffdfba', savefig=False, dpi=100, user='Marius')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calendar Map**  \n",
    "Here you can choose for which year you want to see the activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "years = set(pd.DatetimeIndex(df.Date.values).year)\n",
    "\n",
    "for year in years:\n",
    "    general.calendar_plot(df, year=year, how='count', column='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gs-averages\">2.5 Averages / Summaries</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find general averages and summaries. For those that are interested how often a person said \"I love you\", you can also find that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.print_stats(df, love=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gs-response\">2.6 Response Time</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keep_order = True** maintains the order of messages in the dataframe which is neccesary in order to calculate the response time between messages. However, it is more time-consuming which is why I created a separate function for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.print_timing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"tfidf\">3. TF-IDF</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"tf-count\">3.1 Create Count Vector</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates the dataframe \"counts\" which is a dataframe that contains each word that is said in all conversations and counts how often each user has said that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = tf_idf.count_words_per_user(df, sentence_column=\"Message_Only_Text\", user_column=\"User\")\n",
    "counts = tf_idf.remove_stopwords(counts, language=language, column=\"Word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"tf-unique\">3.2 Unique Words</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see three version of TF-IDF that I created. After much experimentation it turns out that they are all quite similar and that Version C gives me a nice distribution of values needed for the plotting. Content-wise they all have a very similar meaning. \n",
    "\n",
    "**Version A - Messages**  \n",
    "  \n",
    "$TFIDF_i = \\frac{{t_{ij}+1}^2}{\\sum_{i=1}^{n} t_j} \\times \\frac{\\sum_{i=1}^{n} m_i}{m_i} $  \n",
    "  \n",
    "$t_{ij} = $ Number of times word $j$ said by $i$   \n",
    "$m_i = $ Number of messages texted by $i$  \n",
    "  \n",
    "\n",
    "**Version B - Words**\n",
    "\n",
    "$TFIDF_i = \\frac{{t_{ij}+1}^2}{\\sum_{i=1}^{n} t_j} \\times \\frac{\\sum_{i=1}^{n} w_i}{w_i}$\n",
    "\n",
    "$t_{ij} = $ Number of times a specific word $j$ was said by $i$  \n",
    "$w_i = $ Number of words texted by $i$\n",
    "\n",
    "\n",
    "**Version C - Adjusted TF-IDF**\n",
    "\n",
    "$ TFIDF_i = \\frac{{t_{ij} + 1}}{w_i + 1} \\times \\log{\\frac{m}{\\sum_{i=1}^{n} t_j}} $  \n",
    "$w_i = $ Number of words texted by $i$  \n",
    "$t_{ij} = $ Number of times a specific word $j$ was said by $i$  \n",
    "$m = $ Number of all messages   \n",
    "\n",
    "** Unique Words **  \n",
    "  \n",
    "$ Unique_i = \\frac{TFIDF_i}{\\sum\\limits_{j, j \\neq i}^n TFIDF_i} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = tf_idf.get_unique_words(counts, df, version = 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"tf-image\">3.3 Plot Image</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things got a bit tricky...\n",
    "\n",
    "I created a horizontal bar chart with two bars stacked on top of each other both plotted on a background image. I started with a background image and plotted the actual values on the left and made it fully transparent with a white border to separate the bars. Then, on top of that I plotted which bars so that the right part of the image would get removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.print_users(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Make sure to change \"user\" in the function below to one of the users as seen above. It needs to be the exact same naming otherwise it will not work. Furthermore, a valid picture needs to be selected. You can choose the sizing yourself which might need some experimenting with before finding the right size. It also works with a link to the image online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.plot_unique_words(unique_words, \n",
    "                         user='Maria Perez Suarez', \n",
    "                         image_path='../images/mask.png', # use '../images/mask.png' to use the standard image\n",
    "                         image_url=None, \n",
    "                         title=\"Maria Perez Suarez\", \n",
    "                         title_color=\"white\", \n",
    "                         title_background='#AAAAAA', \n",
    "                         width=400, \n",
    "                         height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.plot_unique_words(unique_words, \n",
    "                         user='Marius', \n",
    "                         image_path='../images/mask.png', # use '../images/mask.png' to use the standard image\n",
    "                         image_url=None, \n",
    "                         title=\"Marius\", \n",
    "                         title_color=\"white\", \n",
    "                         title_background='#AAAAAA', \n",
    "                         width=400, \n",
    "                         height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"emoji\">4. Emoji Analysis</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These analysis are based on the Emojis used in each message. Below you can find the following:\n",
    "* Unique Emoji per user\n",
    "* Commonly used Emoji per user\n",
    "* Highly correlated Emoji per user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"emoji-prepare\">4.1 Prepare Data</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "The data needs to be prepared as follows:\n",
    "* Three new columns are created:\n",
    "    * *Emoji* contains all emojis in a message\n",
    "    * *Emoji_Counts* counts the number of emojis in a message\n",
    "    * *Different_Emojis* extracts them in a list for handling purposes\n",
    "* *counts* is a dictionary of dictionaries containing the count of each emoji per user\n",
    "* *unique_emoji* is a dict of dicts containing a uniqueness score of each emoji per user compared to other users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pandas-dev/pandas/issues/17892\n",
    "temp = df[['index', 'Message_Raw', 'User', 'Message_Clean', 'Message_Only_Text']].copy()\n",
    "temp = emoji.prepare_data(temp)\n",
    "\n",
    "# Count all emojis\n",
    "counts = emoji.count_emojis(temp, non_unicode=True)\n",
    "\n",
    "# Get unique emojis\n",
    "list_of_words = [word for user in counts for word in counts[user]]\n",
    "unique_emoji = emoji.get_unique_emojis(temp, counts, list_of_words)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"emoji-stats\">4.2 Unique and Common Emoji</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "Below you can find the most unique emojis according to the TF-IDF and Unique formulas that were used previously: \n",
    "\n",
    "** TF-IDF **\n",
    "\n",
    "$ TFIDF_i = \\frac{{t_{ij} + 1}}{w_i + 1} \\times \\log{\\frac{m}{\\sum_{i=1}^{n} t_j}} $  \n",
    "$w_i = $ Number of words texted by $i$  \n",
    "$t_{ij} = $ Number of times a specific emoji $j$ was said by $i$  \n",
    "$m = $ Number of all messages  \n",
    "\n",
    "** Unique Emoji **  \n",
    "  \n",
    "$ Unique_i = \\frac{TFIDF_i}{\\sum\\limits_{j, j \\neq i}^n TFIDF_i} $\n",
    "\n",
    "I purposefully based it on the number of words and messages in total and only changed *t* to the number of times emoji was said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji.print_stats(unique_emoji, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The often used emoji does not always properly get displayed in matplotlib plots, not sure how to fix this currently..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji.plot_counts(counts, user = user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"emoji-corr\">4.3 Correlation Matrix</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find a correlation matrix of the 15 most often used emoji by a single user. The reasoning for only taking 15 emojis is that otherwise the matrix simply becomes too big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji.plot_corr_matrix(df, user, list_of_words, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"topic\">5. Topic Modelling</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"topic-prepare\">5.1 Prepare Data</a> \n",
    "[Back to Table of Contents](#table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"topic-lda\">5.2 LDA </a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "There is an abundance of methods with regards to topic modelling. Two, however, seem to be the most used: LDA and NMF. Sklearn has an implementation of both LDA and NMF which is why I used that. It should be noted that Gensim is also an extremely good package when it comes to NLP. I will be using Gensim later below for a different application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic.topics(df, model='lda', language=\"french\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"topic-nmf\">5.3 NMF </a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "An application of the NMF model used for topic modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic.topics(df, model='nmf', language=\"french\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"sentiment\">6. Sentiment</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"sentiment-prepare\">6.1 Prepare Data</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "Fortunately, in order to extract sentiment from a Dutch message you can extract the score (-1 being negative and 1 being positive) easily using the pattern package. Below I extract the sentiment as a new column which is needed for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "df['Sentiment'] = df.apply(lambda row: analyser.polarity_scores(row.Message_Clean)[\"compound\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.polarity_scores(\"Great!! It is raining today!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Monsia/camembert-fr-covid-tweet-sentiment-classification\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Monsia/camembert-fr-covid-tweet-sentiment-classification\")\n",
    "\n",
    "nlp_topic_classif = transformers.pipeline('sentiment-analysis', model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_topic_classif(\"Great!! It is raining today!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_topic_classif(\"Super !! Il pleut aujourd'hui !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "prediction = classifier(\"Great!! It is raining today!!!\", )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "prediction = classifier(\"Great!! It is raining today!!!\", )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model='finiteautomata/bertweet-base-sentiment-analysis', return_all_scores=True)\n",
    "prediction = classifier(\"Super... Il pleut aujourd'hui...\", )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"sentiment-average\">6.2 Average Sentiment Per User</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "Prints the average sentiment per user to get an indication of positivity of that user. \n",
    "\n",
    "*NOTE*: THIS ONLY WORKS FOR DUTCH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.print_avg_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"sentiment-plot\">6.3 Plot Sentiment</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "Below you can see a plot of how positive the messages are over time. The y-axis works as follows, a happy smiley indicates a score of 1 (max value) meaning a positive message and a sad smiley indicates a score of -1 (min value) meaning a negative message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.plot_sentiment(df, colors=['#EAAA69','#5361A5'], savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"cloud\">7. Word Clouds</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"cloud-prepare\">7.1 Prepare Data</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts words and create dictionary of words with counts\n",
    "counts = tf_idf.count_words_per_user(df,sentence_column=\"Message_Only_Text\",user_column=\"User\")\n",
    "counts = tf_idf.remove_stopwords(counts, language=\"french\", column=\"Word\")\n",
    "words = counts[[\"Word\", user]].set_index('Word').to_dict()[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"cloud-mask\">7.2 Masked Word Cloud</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.create_wordcloud(words,random_state=42, mask=\"../images/heart.jpg\",\n",
    "                           max_words=1000, max_font_size=50, scale=2, \n",
    "                           normalize_plurals=False, relative_scaling = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"color\">8. Picture Analysis</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "NOTE: This is not part of the whatsapp data. For this chapter to work, you'll need to put at least 1 image in the folder 'Images/' or create a path of your own. It is set to not run automatically since this is merely an extra feature. Set run_colors to True if you want to run the script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"color-get\">8.1 Extract Frequent Colors (k-Means)</a> \n",
    "[Back to Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to retrieve and dump the top 5 most frequent colors in an image using k-Means. \n",
    "It works as follows, each pixel in an image has its own color. That color of each pixel in an image is then extracted in an rgb format. This allows each pixel to have an x (r), y (g) and z (b) coordinate. After extracting all colors of each pixel, k-Means (with k=5) is then used to cluster the points into 5 colors. For each cluster the mediod is calculated and used as a representative of that cluster. Thus, you get top 5 colors for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_colors = True\n",
    "\n",
    "if extract_colors:\n",
    "    # Get Top 5 Frequent Colors\n",
    "    path = '/Users/Marius/Downloads/WhatsApp/'\n",
    "    all_colors = [colors.get_common_colors(path + i) for i in os.listdir(path) if i.endswith('.jpg') #or i.endswith('.png') or i.endswith('.webp')\n",
    "]\n",
    "\n",
    "    # Save colors\n",
    "    with open('colors.pkl', 'wb') as f:\n",
    "        pickle.dump(all_colors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"color-plot\">8.2 Plot Frequent Colors</a> \n",
    "[Back to Table of Contents](#table)\n",
    "\n",
    "After getting the top 5 colors for each picture, they are counted and then displayed below in a histogram. The histogram is in a circular shape where larger/bigger/higher bars indicate colors that are more frequent in the set of pictures. There is a version of the histogram that only shows bars and one that shows a smoothened histogram to be more visually pleasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.plot_color('colors.pkl', smoothen=False, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.plot_color('colors.pkl', smoothen=True, savefig=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}